<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>theory on Shawn T. O&#39;Neil</title>
    <link>/tags/theory/</link>
    <description>Recent content in theory on Shawn T. O&#39;Neil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Mar 2019 00:00:00 +0000</lastBuildDate><atom:link href="/tags/theory/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Random Forests &amp; Gradient Boosting</title>
      <link>/regression-trees-pt2/</link>
      <pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/regression-trees-pt2/</guid>
      <description>In an earlier post we considered machine learning “models” as functions producting predictions from data, and training models to be the production of these functions with higher-order functions. From there we built regression trees–models created recursively by determining 1) a splitting column (column 1 or 2 in this case), and 2) a good value in that column to split the dataset on. At each split, we find a column and value that produces two relatively homogenous sets of y values (in the sense that the values in each y subset can be well-predicted from the column values).</description>
    </item>
    
    <item>
      <title>Regression Trees &amp; Bagging (more functional R)</title>
      <link>/regression-trees/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/regression-trees/</guid>
      <description>I recently ran across this excellent article explaining gradient boosting in the context of regression trees. The article concludes by describing how the technique implements a gradient-descent process, but what I find most fascinating is the concept of “functional modeling”–building machine learning models from other models as building blocks.
This post explores that idea by implementing regression trees in base R (with a little visualization help from ggplot2, dplyr, and tidyr) with functional programming concepts, including a technique called bootstrap aggregating.</description>
    </item>
    
    <item>
      <title>Automatic Differentiation &amp; Functional Operators in R</title>
      <link>/autodifferentation-and-functional-operators-in-r/</link>
      <pubDate>Wed, 08 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/autodifferentation-and-functional-operators-in-r/</guid>
      <description>I’ve been studying up on deep learning recently (I know, trendy), and I learned something along the way that I think is just incredible.1
First, a little background: deep learning models are artificial neural networks, represented as potentially thousands of nodes with millions of weighted connections between them. Input numbers are fed in to some nodes on one side, and out pops output numbers from some nodes on the other side, after winding through the nodes and weighted connections.</description>
    </item>
    
    <item>
      <title>Rstackdeque</title>
      <link>/rstackdeque/</link>
      <pubDate>Sun, 12 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/rstackdeque/</guid>
      <description>I&amp;rsquo;m (usually) a fan of the R programming language, though in a few ways it lacks features computer scientists expect. For example, R lacks many data structures provided by high-level languages, such as trees, queues, and stacks. This is somewhat complicated by R&amp;rsquo;s functional nature&amp;ndash;R programmers expect data structures to be &amp;ldquo;persistent,&amp;rdquo; such that previous versions of the structure are available even after insertions or deletions. Fortunately, data structures in purely-functional languages (generally not R) is a topic of past and ongoing research, spurred initially by Chris Okasaki&amp;rsquo;s excellent Purely Functional Data Structures.</description>
    </item>
    
    <item>
      <title>Topology aware file distribution</title>
      <link>/topology-aware-file-distribution/</link>
      <pubDate>Mon, 25 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/topology-aware-file-distribution/</guid>
      <description>Distributing large data files to all the nodes on a computing network is an important problem in large-scale scientific computing. We developed a more accurate mathematical model for this problem, and although we&amp;rsquo;ve shown minimum-time distribution to be NP-Hard (construction above), we&amp;rsquo;ve also developed a logarithmic approximation solution.
This paper was initially presented at COCOON 2011, and later invited for submission to the Journal of Combinatorial Optimization. Thanks Michael Kowalczyk, NMU alum and current Prof, for the pic of me presenting!</description>
    </item>
    
    <item>
      <title>Algorithmic Haplotyping</title>
      <link>/algorithmic-haplotyping/</link>
      <pubDate>Thu, 12 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>/algorithmic-haplotyping/</guid>
      <description>When working with sequencing datasets of ecological interest, an interesting problem is how to tease out the genetic diversity present in the population being sequenced. Usually, assembly software simply aligns the short read sequences, and determines the consensus sequence based on the majority vote of each position. However, we may wish to seperately assemble each haplotype (version) of each gene.
We formulate this as a graph problem, where short reads that overlap are considered nodes in a graph that share an edge if they should go in different haplotypes.</description>
    </item>
    
    <item>
      <title>Online Learning and the Newsvendor Problem</title>
      <link>/online-learning-and-the-newsvendor-problem/</link>
      <pubDate>Sat, 19 Jan 2008 00:00:00 +0000</pubDate>
      
      <guid>/online-learning-and-the-newsvendor-problem/</guid>
      <description>Machine learning methods and inventory management were a focus in my Master&amp;rsquo;s work. In the newsvendor problem, an amount of product to order must be decided upon periodically for reselling. Ordering too much results in losses due to overstock which must be discarded; too little results in losses due to lost sales.
Traditional approaches experience a tradeoff in that some methods perform better in some situations, while our machine learning approach performs well consistently, particularly in situations where the demand suddenly increases or decreases.</description>
    </item>
    
  </channel>
</rss>
