---
banner: "/regression_trees_pt2/decision_tree_crop.png"
title: Random Forests & Gradient Boosting
author: Shawn T. O'Neil
date: '2019-03-26'
slug: regression-trees-pt2
categories: []
tags:
  - programming
  - r
  - theory
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/viz/viz.js"></script>
<link href="/rmarkdown-libs/DiagrammeR-styles/styles.css" rel="stylesheet" />
<script src="/rmarkdown-libs/grViz-binding/grViz.js"></script>


<p>In an <a href="http://shawntoneil.com/regression-trees/">earlier post</a> we considered machine learning “models” as functions producting predictions from data, and training models to be the production of these functions with higher-order functions. From there we built <em>regression trees</em>–models created recursively by determining 1) a splitting column (column 1 or 2 in this case), and 2) a good value in that column to split the dataset on. At each split, we find a column and value that produces two relatively homogenous sets of <code>y</code> values (in the sense that the values in each <code>y</code> subset can be well-predicted from the column values).</p>
<p>The figure below illustrates splitting on column <code>x3</code>, where values of <code>x3</code> below the threshold produce decent predictions for <code>y</code> as do those above the threshold (better than other potential columns and split points within them). This process repeats for each subset, until either too few rows are left or some other stopping condition is met, in which case the predictions are just simple linear models. When making predictions, we just need to split the query dataset using the same columns and thresholds.</p>
<center>
<img width="80%" src="https://i.imgur.com/yjfv3lp.png" />
</center>
<p>Here we’ll continue the discussion from where we left off: an implementation of regression trees with bootstrap-aggregating, or “bagging,” which produces a number of independent random samples (with replacement) of the training data, trains a model on each, and uses the average model prediction as the overall prediction. This improves the performance of the model by making it more robust to noise in the training data; extreme outliers, for example, may throw off a single model, but amongst the random bootstrap models only some (those whose random samples happened to include the outliers) will be affected.</p>
<!-- ```{r} -->
<!-- train_model <- function(x, y) {  -->
<!--   mean_y <- mean(y)  -->
<!--   predictor <- function(new_x) {  -->
<!--     return(rep(mean_y, length(new_x)))   # give back as many predictions as asked for -->
<!--   } -->
<!--   return(predictor) -->
<!-- } -->
<!-- model <- train_model(x1, y1) -->
<!-- predictions <- model(c(3, 4))  -->
<!-- print(predictions) -->
<!-- ``` -->
<!-- Eventually we produced a version that processed data frame inputs, as a pair of functions. The first, `train_df_index`, produces a model from a given training `df_x` and `y`, as well as which -->
<!-- column of `df_x` should be used for splitting the input into "left" and "right" pieces. Models for -->
<!-- these are computed via `train_df`, which will be assumed to determine the best column to split on as well as do any further work required for those smaller subsets. -->
<!-- As for the predictions themselves, given a `new_df_x` both the left and right models are consulted; -->
<!-- `model_left` predictions are used for rows where the splitting column is less than or equal to the threshold, and `model_right` predictions are used for the rest. -->
<!-- ```{r} -->
<!-- train_df_index <- function(df_x, y, index, depth = 0) { -->
<!--   x_i <- df_x[, index] -->
<!--   if(length(unique(x_i)) < 2 | length(x_i) < 6 | depth == 0) { -->
<!--     return(train_df_index_base(df_x, y, index)) -->
<!--   }  -->
<!--   threshold <- compute_optimal_split(x_i, y) -->
<!--   left_df_x <- df_x[x_i <= threshold, ] -->
<!--   left_y <- y[x_i <= threshold] -->
<!--   right_df_x <- df_x[x_i > threshold, ] -->
<!--   right_y <- y[x_i > threshold] -->
<!--   model_left <- train_df(left_df_x, left_y, depth - 1) -->
<!--   model_right <- train_df(right_df_x, right_y, depth - 1) -->
<!--   predictor <- function(new_df_x) { -->
<!--     predictions_left <- model_left(new_df_x)         # have model_left predict for all new inputs -->
<!--     predictions_right <- model_right(new_df_x)       # have model_right predict for all new inputs -->
<!--     new_x_i <- new_df_x[, index]                     # grab the index'th column -->
<!--     predictions <- rep(0, length(new_x_i)) -->
<!--     predictions[new_x_i <= threshold] <- predictions_left[new_x_i <= threshold] -->
<!--     predictions[new_x_i > threshold] <- predictions_right[new_x_i > threshold] -->
<!--     return(predictions) -->
<!--   } -->
<!--   return(predictor) -->
<!-- } -->
<!-- ``` -->
<!-- The `train_df()` function simply selects the best  -->
<!-- Lastly, we implemented bootstrap-aggregating, or "bagging", which generates a number of random samples (with replacement), builds models trained on each, and averages predictions made by the models. A quick function to generate a bootstrap sample: -->
<!-- ```{r} -->
<!-- get_bootstrap_sample <- function(df_x, y) { -->
<!--   n <- length(y) -->
<!--   sample_indices <- sample(1:n, size = n, replace = TRUE)  # sample the potential indices with replacement -->
<!--   sample_df_x <- df_x[sample_indices, ]                    # grab from the df_x data those rows -->
<!--   sample_y <- y[sample_indices]                            # and from the y data those entries -->
<!--   return(list(sample_df_x, sample_y)) -->
<!-- } -->
<!-- ``` -->
<!-- And, for completeness, the bagged model which collects individual model predictions in a matrix and -->
<!-- uses `apply()` to compute the mean prediction.  -->
<!-- ```{r echo=TRUE, eval=FALSE} -->
<!-- train_df_bootstrap <- function(df_x, y, depth = 0, num_bootstraps = 1) { -->
<!--   models_list <- as.list(rep(NA, num_bootstraps))       # a list of length num_bootstraps (will be a list of models) -->
<!--   for(i in 1:num_bootstraps) { -->
<!--     bootstrap <- get_bootstrap_sample(df_x, y)          # generate bootstrap and pull out sampled df_x and y -->
<!--     bootstrap_df_x <- bootstrap[[1]] -->
<!--     bootstrap_y <- bootstrap[[2]] -->
<!--     model <- train_df(bootstrap_df_x, bootstrap_y, depth)   # train a model -->
<!--     models_list[[i]] <- model                               # store it in the list -->
<!--   } -->
<!--   predictor <- function(new_df_x) { -->
<!--     all_predictions <- matrix(0, nrow = nrow(new_df_x), ncol = num_bootstraps)  # num_predictions X num_models -->
<!--     for(i in 1:num_bootstraps) { -->
<!--       model <- models_list[[i]]                    # get i'th model -->
<!--       model_predictions <- model(new_df_x) -->
<!--       all_predictions[, i] <- model_predictions    # store predictions in i'th column -->
<!--     } -->
<!--     predictions <- apply(all_predictions, MARGIN = 1, mean) # average columns -->
<!--     return(predictions) -->
<!--   } -->
<!--   return(predictor) -->
<!-- } -->
<!-- ``` -->
<p>Although both bagging and tree-based models can be used for discrete class predictions (classification), we are working with continuous predictions (regression)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. As a reminder, here is the dataset we are training on predicting on, where continuous <code>y</code> values are based on <code>x1</code> and <code>x2</code> values in a complex way:</p>
<pre class="r"><code>x1_values &lt;- seq(-1, 2, length.out = 30)
x2_values &lt;- seq(-0.5, 2.5, length.out = 30)

# data frame of predictor variables; all combinations of x1_values and x2_values
df_x &lt;- expand.grid(x1 = x1_values, x2 = x2_values)

# the y response vector depends on both
y &lt;- cos(0.8*df_x$x1 + 0.2*df_x$x2) ^ 3 + cos(df_x$x2) ^ 3

# we&#39;ll center the y vector to a mean of 0 for illustration
y &lt;- y - mean(y)


# a data frame of x1, x2, and y
train_data &lt;- data.frame(df_x, y)

ggplot(train_data) +
  geom_tile(aes(x = x1, y = x2, fill = y, color = y)) +
  geom_contour(aes(x = x1, y = x2, z = y), bins = 10) + 
  coord_equal() + 
  scale_fill_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;) +
  scale_color_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;) </code></pre>
<p><img src="figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p><br /><b></b></p>
<div id="random-forests" class="section level2">
<h2>Random Forests</h2>
<p>Bootstrap-aggregating is curious, in that each individual bootstrap model should, in theory, be less powerful than a model that considers all of the data. And yet, by combining predictions from a variety of these weaker models, we get better results overall. Such methods are known as “ensemble” methods.</p>
<p>Ensemble methods work best when the various models being aggregated are <em>not</em> highly correlated. Consider what would happen if each bootstrap model happened to get the same random sample: they would all produce identical predictions, negating the benefit. That’s not to say that we want the predictions made by the models to be more random, but rather that we want all of the predictions to be accurate, just generated in different ways by the various models. An ensemble of similar predictions generated by very different means is likely to be robust to new <span class="math inline">\(x\)</span> data, whereas an ensemble of different predictions produced in very similar ways is, well, just noise.</p>
<p>Ensemble methods work well then, when each individual model produces good (or at least decent, and hopefully non-biased) predictions, but the models themselves are quite different. Bootstrap aggregating accomplishes this by subsetting random training data. Another potential way to do this might be to aggregate predictions from the various models, but choose the depths (how many times we allow a split) randomly.</p>
<p>But some methods of generating these sets of models are better than others. Consider the <code>depth = 2</code> (where the data are split, and then each subset is split, but it ends there), <code>num_boostraps = 30</code> panel in the lower-left below. Here the models are trained on a noisy version of the <code>y</code> data.</p>
<p><img src="figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Even though each model is trained on random data, they almost universally choose to split at or near the same points, in the same order (split first on <code>x1</code> near 1.5, then for the “left” model split again on <code>x1</code> near -1.5 while the “right” splits on <code>x2</code> near -0.5). The various models are highly correlated, because in this data <code>x1</code> is the obvious best choice for the first split, even on a random subset of data.</p>
<p>A popular modification of the bootstrap technique generates what are known as Random Forests. Here, in addition to generating random subsets of data, each regression tree only considers a <em>random subset of columns</em> when determining the best column to split on.</p>
<p>Consider what would happen if some of the models were disallowed <code>x1</code> as the first splitting column. They would be forced to find the best split point on the remaining column <code>x2</code>, but then of course for later splits could use <code>x1</code> (or not, depending on what restrictions we place on future splits). These models may be weaker than a model without such restrictions, but the predictions made would still be pretty good. Importantly, the various models would use very different processes to arrive at their predictions, and collectively they would be robust to some columns hogging all of the attention.</p>
<p>The Random Forest algorithm implements this idea by modifying the split procedure. Whenever the best column for splitting is being determined, only a random subset of potential splitting columns is considered. This adds random restrictions at each split point, generating a large number of “random trees,” hence the name Random Forest.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> In our framework implementing a Random Forest is as easy as modifying the <code>train_df</code> function to consider a random subset of columns when determining the best column to split on. If there are <span class="math inline">\(n\)</span> column indices to check, we’ll select <span class="math inline">\(\lfloor n/2 \rfloor\)</span> at random.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<pre class="r"><code>train_df &lt;- function(df_x, y, depth = 0) {
  error_func &lt;- function(x) {
    model &lt;- train_optimal_split_lm(x, y)
    return(sse(y, model(x)))
  }

  best_index &lt;- Inf
  best_error &lt;- Inf
  
  #for(index in 1:ncol(df_x)) {                               # instead of checking all column indices 
  num_indices &lt;- as.integer(ncol(df_x) / 2)                   # n/2, rounded down
  random_indices &lt;- sample(1:ncol(df_x), size = num_indices)  # which column indices can we consider?
  for(index in random_indices) {                              # only check those indices 
    x_i &lt;- df_x[ , index]
    error &lt;- error_func(x_i)
    if(error &lt; best_error) {
      best_index &lt;- index
      best_error &lt;- error
    }
  }

  return(train_df_index(df_x, y, best_index, depth))
}</code></pre>
<p>Let’s re-run the depth-vs-num_bootstraps analysis from above with this new change.</p>
<p><img src="figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The difference isn’t dramatic, but note the additional gradations present in the lower-left panel: rather than four very homogenous quadrants, each contains some additional nuances that better reflect the trends in the training data. Random Forests can be particularly helpful when the number of training columns is large, since in many real datasets a few “important” columns can dominate the tree-making process, leaving columns of lesser importance (but still useful!) out in the cold.</p>
<p><br /><b></b></p>
</div>
<div id="gradient-boosting" class="section level2">
<h2>Gradient Boosting</h2>
<p>Our last method will also be based on the <code>train_df</code> function. While random-column selection is compatible, it’s not usually incorporated, we’ll revert to the earlier version. We’ll also be training on our non-noised <code>y</code> data, to better visualize what’s going on.</p>
<p>To introduce gradient boosting, we’ll start with a relatively weak model that only goes 3 splits deep (<code>depth = 3</code>). Here’s one predicting our <code>y</code> data.</p>
<pre class="r"><code># generate predictions without allowing slope
USE_SLOPE &lt;- FALSE
base_model &lt;- train_df(df_x, y, depth = 3)
yhat &lt;- base_model(df_x)

# data frame with columns for x1, x2, y, and yhat
all_data &lt;- data.frame(df_x, y, yhat)

ggplot(all_data) +
  geom_tile(aes(x = x1, y = x2, fill = yhat, color = yhat)) +
  geom_contour(aes(x = x1, y = x2, z = y), bins = 10) +
  coord_equal() +
  scale_fill_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;) +
  scale_color_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;)</code></pre>
<p><img src="figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Similarly, we can plot and compute the differences between <code>yhat</code> and <code>y</code> (the residuals, which I like to think of as “prediction errors,” but shouldn’t be confused with sum-absolute-error or sum-squared-error used for model training).</p>
<pre class="r"><code>residuals &lt;- y - yhat
print(head(residuals))</code></pre>
<pre><code>## [1] -0.7224069 -0.6422521 -0.5537846 -0.4600621 -0.3646723 -0.2715274</code></pre>
<pre class="r"><code># data frame with columns for x1, x2, y, and residuals
all_data &lt;- data.frame(df_x, y, residuals)

ggplot(all_data) +
  geom_tile(aes(x = x1, y = x2, fill = residuals, color = residuals)) +
  geom_contour(aes(x = x1, y = x2, z = y), bins = 10) +
  coord_equal() +
  scale_fill_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;) +
  scale_color_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;)</code></pre>
<p><img src="figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Now, what would happen if we were to try to produce a model that predicts not <code>y</code>, but rather the <code>residuals</code>? If we could predict the residuals perfectly, all we need to do is add them to the not-so-great <code>yhat</code> values to get back the real <code>y</code> values. We’re going generate a model that predicts how to correct the first model! But we’re going to do this with another rather weak regression tree of depth 3.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> Here’s what the predicted residuals look like from this training:</p>
<pre class="r"><code>corrector &lt;- train_df(df_x, residuals, depth = 3)
residuals_hat &lt;- corrector(df_x)

# data frame with columns for x1, x2, y, residuals_hat
all_data &lt;- data.frame(df_x, y, residuals_hat)

ggplot(all_data) +
  geom_tile(aes(x = x1, y = x2, fill = residuals_hat, color = residuals_hat)) +
  geom_contour(aes(x = x1, y = x2, z = y), bins = 10) +
  coord_equal() +
  scale_fill_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;) +
  scale_color_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;)</code></pre>
<p><img src="figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Since the above are the predictions for the residuals, if we want to make updated predictions for the original <code>y</code> data we need to add these to <code>yhat</code> to produce new <code>yhat</code> predictions.</p>
<pre class="r"><code>yhat &lt;- yhat + residuals_hat

# data frame with columns for x1, x2, y, and new yhat
all_data &lt;- data.frame(df_x, y, yhat)

ggplot(all_data) +
  geom_tile(aes(x = x1, y = x2, fill = yhat, color = yhat)) +
  geom_contour(aes(x = x1, y = x2, z = y), bins = 10) +
  coord_equal() +
  scale_fill_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;) +
  scale_color_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;)</code></pre>
<p><img src="figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>In the next iteration we repeat: we compute a new <code>residuals</code> from the current <code>yhat</code> and <code>y</code>, train another corrector model on those, and our new <code>yhat</code> will be the sum:</p>
<pre class="r"><code>residuals &lt;- y - yhat
corrector &lt;- train_df(df_x, residuals, depth = 3)
residuals_hat &lt;- corrector(df_x)
yhat &lt;- yhat + residuals_hat

# data frame with columns for x1, x2, y, residuals_hat
all_data &lt;- data.frame(df_x, y, residuals_hat)

ggplot(all_data) +
  geom_tile(aes(x = x1, y = x2, fill = yhat, color = yhat)) +
  geom_contour(aes(x = x1, y = x2, z = y), bins = 10) +
  coord_equal() +
  scale_fill_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;) +
  scale_color_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;)</code></pre>
<p><img src="figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>This process, repeated, is the essense of gradient boosting: during training, each model is trained on the error of the last. When making new predictions, we simply need to add up all the predictions of the various models.</p>
<p>Although we could use a loop for this, a recursive definition works well too. The <code>stack_size</code> parameter controls how many models are built (since we are “stacking” their predictions); if <code>stack_size = 0</code> a very simple base model used,<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> otherwise, a model with <code>stack_size - 1</code> is used for the initial <code>yhat</code> predictions (which will contain <code>stack_size - 1</code> models), and to this we add predictions for residuals.</p>
<pre class="r"><code>train_df_boosted &lt;- function(df_x, y, stack_size = 0) {
  # stack_size = 0 is just the mean of the y values
  if(stack_size == 0) {
    predictor &lt;- train_df(df_x, y, depth = 0)
    return(predictor)
  }
  
  # train a model to estimate y
  submodel &lt;- train_df_boosted(df_x, y, stack_size - 1)
  
  # compute residuals
  yhat &lt;- submodel(df_x)
  residuals &lt;- y - yhat
  
  # build a model that predicts the residuals
  corrector &lt;- train_df(df_x, residuals, depth = 3)
  
  # overall prediction: submodel predictions + corrector predictions
  predictor &lt;- function(new_df_x) {
    submodel_predictions &lt;- submodel(new_df_x)
    corrector_predictions &lt;- corrector(new_df_x)
    return(submodel_predictions + corrector_predictions)
  }
  
  return(predictor)
}</code></pre>
<p>Let’s see how this scheme performs with different <code>depth</code> values.</p>
<p><img src="figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Notice how quickly the model coverges with even small stacks of models! This is actually a deficit of gradient boosting: it’s so powerful that it can be quick to learn both the trends in the data and the noise. Here’s the same plot, but trained on the <code>ynoised</code> data.</p>
<p><img src="figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>To mitigate this, another parameter is added, <span class="math inline">\(\eta\)</span> (<code>eta</code>). Rather than adding the full estimated corrections supplied by each corrector model, we tone these down by multiplying them by this small constant, usually in the range 0.1 to 0.3.</p>
<pre class="r"><code>train_df_boosted &lt;- function(df_x, y, stack_size = 0, eta = 0.1) {
  # depth = 0 is just a single decision stump
  if(stack_size == 0) {
    #predictor &lt;- train_df(df_x, y, depth = 4)
    predictor &lt;- function(new_df_x) {
      return(rep(mean(y), nrow(new_df_x)))
    }
    return(predictor)
  }
  
  # train a model to estimate y
  submodel &lt;- train_df_boosted(df_x, y, stack_size - 1, eta)    # pass it down
  
  # compute residuals
  yhat &lt;- submodel(df_x)
  residuals &lt;- y - yhat
  
  # build a model that predicts the residuals
  corrector &lt;- train_df(df_x, residuals, depth = 3)
  
  # overall prediction: submodel predictions + corrector predictions
  predictor &lt;- function(new_df_x) {
    submodel_predictions &lt;- submodel(new_df_x)
    corrector_predictions &lt;- corrector(new_df_x)
    return(submodel_predictions + eta * corrector_predictions)  # multiply by eta
  }
  
  return(predictor)
}</code></pre>
<p>This adjustment requires significantly more models for the same performance, so the noise-robustness comes at a computational cost.</p>
<p><img src="figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p><br /><b></b></p>
</div>
<div id="wrapup" class="section level2">
<h2>Wrapup</h2>
<p>Gradient-boosting is a rather elegant method for building complex models from simpler ones, and it lends itself nicely to a functional, recursively-defined implementation. Gradient boosting, by the way, is so-called because the “gradient” is the direction of most change (toward improving a cost function, like sum-squared-error), and each model attempts to predict the error of the stack thus far, directly attempting to improve the model by reducing error in an efficient way. It isn’t just intuitive, it has a strong theoretical basis as a <a href="https://explained.ai/gradient-boosting/descent.html">gradient-descent process</a>. While some argue there’s a particular beauty in that the gradient is descended in “function space” rather than “parameter space,” I’m not sure I see that, given the correspondence between predictive models and functions we’ve explored.</p>
<p><br /><b></b></p>
</div>
<div id="bonus" class="section level2">
<h2>Bonus?</h2>
<p>I did attempt to put together a more compact regression-tree model with some improvements, though it’s not the prettiest.</p>
<p>First, I realized that training could handle discrete columns easily if these are converted to ordered factors, with the ordering determined by the variance of <code>y</code> across categories. This lets us work with factors much like numbers, and puts the smallest-variance category “first”, allowing a split to separate out the category with least variance (or, if it may be better, to split into groups of smaller-variance categories and larger-variance categories).</p>
<pre class="r"><code>order_categoricals_by_variance &lt;- function(df, y) {
  for(i in 1:ncol(df)) {
    col_i &lt;- df[, i]
  
    if(!is.numeric(col_i)) {
      # order the levels in col_i by data in y, using var() as the function to order by
      df[, i] &lt;- reorder(col_i, y, var, order = TRUE)
    }
    
  }
  
  return(df)
}</code></pre>
<p>We also need a way to copy this order to a <code>new_df_x</code> when it comes time to make predictions.</p>
<pre class="r"><code>order_categoricals_copy &lt;- function(df, copy_from_df) {
  for(i in 1:ncol(df)) {
    col_i &lt;- df[, i]
    from_i &lt;- copy_from_df[, i]
    
    if(!is.numeric(col_i)) {
      # use the same levels and ordering as in from_i
      df[, i] &lt;- factor(col_i, levels = levels(from_i), ordered = TRUE)
    }
    
  }
  
  return(df)
}</code></pre>
<p>A simpler base-predictor always uses the mean of the <code>y</code> data; note that here the predictor returns <code>new_df_x</code> with a new <code>yhat</code> column; this allows us to make predictions on splitted subsets of data rather than the entire query dataset as the previous code did.</p>
<pre class="r"><code>train_base &lt;- function(df_x, y) {
  mean_y &lt;- mean(y)

  predictor &lt;- function(new_df_x) {
    new_df_x$yhat &lt;- rep(mean_y, nrow(new_df_x))
    return(new_df_x)
  }
  
  return(predictor)
}</code></pre>
<p>A <code>split_df</code> function does the job of splitting a dataframe into two pieces, by finding the best splitting column and best split-point (considering all possibilities). It returns a list with those two pieces, correspinding <code>y</code> values, and the column index and value it found the split on. Additionally, the last element of the returned list contains labels for the splitting criteria, e.g. <code>c(&quot;x2 &lt;= 1.88&quot;, &quot;x2 &gt; 1.88&quot;)</code> or <code>c(&quot;x4 is Yes or Maybe&quot;, &quot;x4 is No or Never&quot;)</code> in the case of categorical variables.</p>
<pre class="r"><code>split_df &lt;- function(df_x, y) {
  best_sse &lt;- Inf
  best_i &lt;- NA
  best_threshold &lt;- NA
  
  for(i in 1:ncol(df_x)) {
    x_i &lt;- df_x[, i]
    
    for(threshold in unique(x_i)) {
      residuals_lte &lt;- y[x_i &lt;= threshold] - mean(y[x_i &lt;= threshold])
      residuals_gt &lt;- y[x_i &gt; threshold] - mean(y[x_i &gt; threshold])

      sse &lt;- sum(residuals_lte ^ 2) + sum(residuals_gt ^ 2)
      
      # sse could be NA if y[x_i &gt; threshold] has no data; we won&#39;t consider these non-split-splits
      if(!is.na(sse) &amp; sse &lt; best_sse) {
        best_sse &lt;- sse
        best_i &lt;- i
        best_threshold &lt;- threshold
      }
    }
  }
  
  best_x_i &lt;- df_x[, best_i]
  
  df_x_lte &lt;- df_x[best_x_i &lt;= best_threshold, ]
  y_lte &lt;- y[best_x_i &lt;= best_threshold]
  
  df_x_gt &lt;- df_x[best_x_i &gt; best_threshold, ]
  y_gt &lt;- y[best_x_i &gt; best_threshold]
  
  split_labels &lt;- create_split_labels(df_x, best_i, best_threshold)
  
  parts &lt;- list(df_x_lte, df_x_gt, y_lte, y_gt, best_i, best_threshold, split_labels)
  return(parts)
}</code></pre>
<p>Creating the split labels is messy work, particularly for categorical splits.</p>
<pre class="r"><code>create_split_labels &lt;- function(df_x, best_i, best_threshold) {
  
  if(is.numeric(best_threshold)) {
    best_threshold &lt;- signif(best_threshold, 3)
    split_labels &lt;- c(paste0(colnames(df_x)[best_i], &quot; &lt;= &quot;, best_threshold), 
                       paste0(colnames(df_x)[best_i], &quot; &gt; &quot;, best_threshold))
  } else {
    best_x_i &lt;- df_x[, best_i]
    present_levels &lt;- levels(best_x_i)[levels(best_x_i) %in% unique(best_x_i)]
    lte_levels &lt;- paste(present_levels[present_levels &lt;= best_threshold], collapse = &quot; or &quot;)
    gt_levels &lt;- paste(present_levels[present_levels &gt; best_threshold], collapse = &quot; or &quot;)
    split_labels &lt;- c(paste0(colnames(df_x)[best_i], &quot; is &quot;, lte_levels), 
                       paste0(colnames(df_x)[best_i], &quot; is &quot;, gt_levels))
  }
  
  return(split_labels)
}</code></pre>
<p>Lastly, the <code>train_tree</code> function; note that at each level we can re-order the factor levels so that levels are ordered by variance per subset of the data; when making predictions, the same ordering is used.</p>
<pre class="r"><code>train_tree &lt;- function(df_x, y, depth = 0) {
  if(depth == 0 | nrow(df_x) &lt;= 6) {
    return(train_base(df_x, y))
  }

  # recompute level ordering for this subset of data
  df_x &lt;- order_categoricals_by_variance(df_x, y)
  
  split_parts &lt;- split_df(df_x, y)
  
  left_df_x &lt;- split_parts[[1]]
  right_df_x &lt;- split_parts[[2]]
  
  left_y &lt;- split_parts[[3]]
  right_y &lt;- split_parts[[4]]
  
  best_i &lt;- split_parts[[5]]
  best_threshold &lt;- split_parts[[6]]
  split_labels &lt;- split_parts[[7]]
  
  left_model &lt;- train_tree(left_df_x, left_y, depth - 1)
  right_model &lt;- train_tree(right_df_x, right_y, depth - 1)
  
  predictor &lt;- function(new_df_x) {
    # use the same level ordering as was computed for training
    new_df_x &lt;- order_categoricals_copy(new_df_x, df_x)
    new_x_i &lt;- new_df_x[, best_i]
    
    left_new_df_x &lt;- new_df_x[new_x_i &lt;= best_threshold, ]
    right_new_df_x &lt;- new_df_x[new_x_i &gt; best_threshold, ]
    
    # answer is a list of 2 model answers
    answer &lt;- list(left_model(left_new_df_x), right_model(right_new_df_x))
    names(answer) &lt;- split_labels
      
    return(answer)
  }
  
  return(predictor)
}</code></pre>
<p>An interesting feature of the above is that it always returns a list as an answer, containing the two answers from the sub-models, and we name the list elements with the splitting labels. This creates a nested data structure containing the predictions as well as a hierarchical clustering of the input data based on the model structure. Here’s a small test dataset that includes two categorical variables, <code>grade</code> and <code>check</code>.</p>
<pre class="r"><code>x1_values &lt;- seq(-1, 2, length.out = 30)
x2_values &lt;- seq(-0.5, 2.5, length.out = 30)
grade_values &lt;-c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)
check_values &lt;-c(&quot;YES&quot;, &quot;NO&quot;, &quot;MAYBE&quot;)

# data frame of predictor variables; all combinations of x1_values and x2_values
df_x &lt;- expand.grid(x1 = x1_values, x2 = x2_values, grade = grade_values, check = check_values)
grade_adj &lt;- as.integer(as.factor(df_x$grade)) - mean(as.integer(as.factor(df_x$grade))) # adjustment for grade

# the y response vector depends on both
y &lt;- cos(0.8*df_x$x1 + 0.2*df_x$x2) ^ 3 + cos(df_x$x2 + grade_adj/3) ^ 3 

# adjustment for check column
y[df_x$check == &quot;MAYBE&quot;] &lt;- -0.75 * y[df_x$check == &quot;MAYBE&quot;]
y[df_x$check == &quot;YES&quot;] &lt;- -1.5 * y[df_x$check == &quot;YES&quot;]

# we&#39;ll center the y vector to a mean of 0 for illustration
y &lt;- y - mean(y)

# a data frame of x1, x2, and y
train_data &lt;- data.frame(df_x, y)

ggplot(train_data) +
  geom_tile(aes(x = x1, y = x2, fill = y, color = y)) +
  geom_contour(aes(x = x1, y = x2, z = y), bins = 10) + 
  coord_equal() + 
  scale_fill_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;) +
  scale_color_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;) +
  facet_grid(check ~ grade)</code></pre>
<p><img src="figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<pre class="r"><code>print(head(train_data))</code></pre>
<pre><code>##           x1   x2 grade check          y
## 1 -1.0000000 -0.5     A   YES -0.2883904
## 2 -0.8965517 -0.5     A   YES -0.4086226
## 3 -0.7931034 -0.5     A   YES -0.5413238
## 4 -0.6896552 -0.5     A   YES -0.6819075
## 5 -0.5862069 -0.5     A   YES -0.8249922
## 6 -0.4827586 -0.5     A   YES -0.9647095</code></pre>
<p>And here’s what the prediction structure looks like for a depth-3 model:</p>
<pre class="r"><code>model &lt;- train_tree(df_x, y, depth = 4);
result &lt;- model(df_x)
str(result, give.attr = FALSE)</code></pre>
<pre><code>## List of 2
##  $ check is MAYBE or NO:List of 2
##   ..$ check is MAYBE:List of 2
##   .. ..$ x2 &lt;= 0.948:List of 2
##   .. .. ..$ x1 &lt;= 0.759:&#39;data.frame&#39;:    1080 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:1080] -1 -0.897 -0.793 -0.69 -0.586 ...
##   .. .. .. ..$ x2   : num [1:1080] -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;MAYBE&quot;&lt;&quot;NO&quot;&lt;&quot;YES&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:1080] -0.747 -0.747 -0.747 -0.747 -0.747 ...
##   .. .. ..$ x1 &gt; 0.759 :&#39;data.frame&#39;:    720 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:720] 0.862 0.966 1.069 1.172 1.276 ...
##   .. .. .. ..$ x2   : num [1:720] -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;MAYBE&quot;&lt;&quot;NO&quot;&lt;&quot;YES&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:720] -0.264 -0.264 -0.264 -0.264 -0.264 ...
##   .. ..$ x2 &gt; 0.948 :List of 2
##   .. .. ..$ x1 &lt;= 0.345:&#39;data.frame&#39;:    840 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:840] -1 -0.897 -0.793 -0.69 -0.586 ...
##   .. .. .. ..$ x2   : num [1:840] 1.05 1.05 1.05 1.05 1.05 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;MAYBE&quot;&lt;&quot;NO&quot;&lt;&quot;YES&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:840] -0.237 -0.237 -0.237 -0.237 -0.237 ...
##   .. .. ..$ x1 &gt; 0.345 :&#39;data.frame&#39;:    960 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:960] 0.448 0.552 0.655 0.759 0.862 ...
##   .. .. .. ..$ x2   : num [1:960] 1.05 1.05 1.05 1.05 1.05 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;MAYBE&quot;&lt;&quot;NO&quot;&lt;&quot;YES&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:960] 0.321 0.321 0.321 0.321 0.321 ...
##   ..$ check is NO   :List of 2
##   .. ..$ x2 &lt;= 0.948:List of 2
##   .. .. ..$ x1 &lt;= 0.759:&#39;data.frame&#39;:    1080 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:1080] -1 -0.897 -0.793 -0.69 -0.586 ...
##   .. .. .. ..$ x2   : num [1:1080] -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;NO&quot;&lt;&quot;MAYBE&quot;&lt;&quot;YES&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:1080] 1.72 1.72 1.72 1.72 1.72 ...
##   .. .. ..$ x1 &gt; 0.759 :&#39;data.frame&#39;:    720 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:720] 0.862 0.966 1.069 1.172 1.276 ...
##   .. .. .. ..$ x2   : num [1:720] -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;NO&quot;&lt;&quot;MAYBE&quot;&lt;&quot;YES&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:720] 1.07 1.07 1.07 1.07 1.07 ...
##   .. ..$ x2 &gt; 0.948 :List of 2
##   .. .. ..$ x1 &lt;= 0.345:&#39;data.frame&#39;:    840 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:840] -1 -0.897 -0.793 -0.69 -0.586 ...
##   .. .. .. ..$ x2   : num [1:840] 1.05 1.05 1.05 1.05 1.05 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;NO&quot;&lt;&quot;MAYBE&quot;&lt;&quot;YES&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:840] 1.04 1.04 1.04 1.04 1.04 ...
##   .. .. ..$ x1 &gt; 0.345 :&#39;data.frame&#39;:    960 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:960] 0.448 0.552 0.655 0.759 0.862 ...
##   .. .. .. ..$ x2   : num [1:960] 1.05 1.05 1.05 1.05 1.05 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;NO&quot;&lt;&quot;MAYBE&quot;&lt;&quot;YES&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:960] 0.291 0.291 0.291 0.291 0.291 ...
##  $ check is YES        :List of 2
##   ..$ x2 &lt;= 0.948:List of 2
##   .. ..$ x1 &lt;= 0.759:List of 2
##   .. .. ..$ x2 &lt;= 0.534:&#39;data.frame&#39;:    792 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:792] -1 -0.897 -0.793 -0.69 -0.586 ...
##   .. .. .. ..$ x2   : num [1:792] -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;YES&quot;&lt;&quot;MAYBE&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:792] -1.93 -1.93 -1.93 -1.93 -1.93 ...
##   .. .. ..$ x2 &gt; 0.534 :&#39;data.frame&#39;:    288 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:288] -1 -0.897 -0.793 -0.69 -0.586 ...
##   .. .. .. ..$ x2   : num [1:288] 0.638 0.638 0.638 0.638 0.638 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;YES&quot;&lt;&quot;MAYBE&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:288] -1.45 -1.45 -1.45 -1.45 -1.45 ...
##   .. ..$ x1 &gt; 0.759 :List of 2
##   .. .. ..$ x2 &lt;= 0.431:&#39;data.frame&#39;:    480 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:480] 0.862 0.966 1.069 1.172 1.276 ...
##   .. .. .. ..$ x2   : num [1:480] -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;YES&quot;&lt;&quot;MAYBE&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:480] -1.02 -1.02 -1.02 -1.02 -1.02 ...
##   .. .. ..$ x2 &gt; 0.431 :&#39;data.frame&#39;:    240 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:240] 0.862 0.966 1.069 1.172 1.276 ...
##   .. .. .. ..$ x2   : num [1:240] 0.534 0.534 0.534 0.534 0.534 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;YES&quot;&lt;&quot;MAYBE&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:240] -0.469 -0.469 -0.469 -0.469 -0.469 ...
##   ..$ x2 &gt; 0.948 :List of 2
##   .. ..$ x1 &lt;= 0.345:List of 2
##   .. .. ..$ x2 &lt;= 1.88:&#39;data.frame&#39;: 504 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:504] -1 -0.897 -0.793 -0.69 -0.586 ...
##   .. .. .. ..$ x2   : num [1:504] 1.05 1.05 1.05 1.05 1.05 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;YES&quot;&lt;&quot;MAYBE&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:504] -1.03 -1.03 -1.03 -1.03 -1.03 ...
##   .. .. ..$ x2 &gt; 1.88 :&#39;data.frame&#39;: 336 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:336] -1 -0.897 -0.793 -0.69 -0.586 ...
##   .. .. .. ..$ x2   : num [1:336] 1.98 1.98 1.98 1.98 1.98 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;YES&quot;&lt;&quot;MAYBE&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:336] -0.414 -0.414 -0.414 -0.414 -0.414 ...
##   .. ..$ x1 &gt; 0.345 :List of 2
##   .. .. ..$ x2 &lt;= 1.88:&#39;data.frame&#39;: 576 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:576] 0.448 0.552 0.655 0.759 0.862 ...
##   .. .. .. ..$ x2   : num [1:576] 1.05 1.05 1.05 1.05 1.05 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;YES&quot;&lt;&quot;MAYBE&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:576] 0.068 0.068 0.068 0.068 0.068 ...
##   .. .. ..$ x2 &gt; 1.88 :&#39;data.frame&#39;: 384 obs. of  5 variables:
##   .. .. .. ..$ x1   : num [1:384] 0.448 0.552 0.655 0.759 0.862 ...
##   .. .. .. ..$ x2   : num [1:384] 1.98 1.98 1.98 1.98 1.98 ...
##   .. .. .. ..$ grade: Ord.factor w/ 4 levels &quot;B&quot;&lt;&quot;A&quot;&lt;&quot;C&quot;&lt;&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##   .. .. .. ..$ check: Ord.factor w/ 3 levels &quot;YES&quot;&lt;&quot;MAYBE&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ...
##   .. .. .. ..$ yhat : num [1:384] 0.733 0.733 0.733 0.733 0.733 ...</code></pre>
<p>A little function to collate all the sub-results into a single dataframe:</p>
<pre class="r"><code>flatten_result &lt;- function(res) {
  if(is.data.frame(res)) {
    # base case: just return the dataframe
    return(res)
  }
  
  # recursive cases: get dataframes from the sub-answers and rbind them
  sub1 &lt;- flatten_result(res[[1]])
  sub2 &lt;- flatten_result(res[[2]])
  return(rbind(sub1, sub2))
}

result_flattened &lt;- flatten_result(result)
print(head(result_flattened))</code></pre>
<pre><code>##              x1   x2 grade check       yhat
## 7201 -1.0000000 -0.5     A MAYBE -0.7473205
## 7202 -0.8965517 -0.5     A MAYBE -0.7473205
## 7203 -0.7931034 -0.5     A MAYBE -0.7473205
## 7204 -0.6896552 -0.5     A MAYBE -0.7473205
## 7205 -0.5862069 -0.5     A MAYBE -0.7473205
## 7206 -0.4827586 -0.5     A MAYBE -0.7473205</code></pre>
<p>The <code>data.tree</code> package provides a nice way to visualize named nested lists; first we’ll create a version of the result where the bottom dataframes are replaced with lists that have names indicating what the predicted value should be (since <code>data.tree</code> visualizes the names of nested lists, not their contents).<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<pre class="r"><code>library(data.tree)

flatten_result_convert &lt;- function(res) {
  if(is.data.frame(res)) {
    # base case: return a list with a name describing the dataframe predictions
    return_list &lt;- list(list(&quot;placeholder&quot;))   # data.tree doesn&#39;t want to show the leaves?
    names(return_list) &lt;- paste0(&quot;predict &quot;, signif(res$yhat[1], 3))
    return(return_list)
  }
  
  # recursive cases: fix up the sub-answers, re-listify them with the correct names
  sub1 &lt;- flatten_result_convert(res[[1]])
  sub2 &lt;- flatten_result_convert(res[[2]])
  to_return &lt;- list(sub1, sub2)
  names(to_return) &lt;- names(res)
  return(to_return)
}

# convert the nested list to a data.tree (with as.Node()) and plot it
decision_tree &lt;- as.Node(flatten_result_convert(result))
SetGraphStyle(decision_tree, rankdir = &quot;LR&quot;)
SetNodeStyle(decision_tree, style = &quot;filled,rounded&quot;, shape = &quot;box&quot;, fillcolor = &quot;steelblue4&quot;, fontname = &quot;helvetica&quot;)
plot(decision_tree)</code></pre>

<img src="/regression_trees_pt2/decision_tree.png" />
<br />
<br />


<p>Visualizing the predictions for different model depths, with a little help from <code>purrr</code> and <code>gganimate</code>:</p>
<pre class="r"><code>library(gganimate)
library(purrr)

depths &lt;- as.list(0:20)
# build a version of train_tree with df_x and y already specified
train_tree_preconfigured &lt;- partial(train_tree, df_x, y)
# build a list of models by calling train_tree_preconfigured on each element of depths
models &lt;- map(depths, train_tree_preconfigured)
# call each model on df_x (contained in a list to make it a single element) getting a list of results
# (I&#39;m never able to get invoke_map to work right, map2 with an &quot;applicator&quot; function works though)
results &lt;- map2(list(df_x), models, function(df, model) {return(model(df))})
# flatten each
flattened_results &lt;- map(results, flatten_result)
# add a column for &quot;depth&quot; to each so we know what the depths used were
flattened_results &lt;- map2_df(flattened_results, depths, function(result_df, depth) {
                                                          result_df$depth &lt;- depth
                                                          return(result_df)
                                                        })

p &lt;- ggplot(flattened_results) +
     geom_tile(aes(x = x1, y = x2, fill = yhat, color = yhat, group = depth)) +
     coord_equal() + 
     scale_fill_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;) +
     scale_color_gradient2(low = &quot;purple4&quot;, high = &quot;orange3&quot;) +
     facet_grid(check ~ grade) +
     # gganimate parts
     labs(title = &#39;model depth: {closest_state}&#39;) +
     transition_states(depth) 

options(gganimate.dev_args = list(width = 800, height = 600), nframes = 20, fps = 2)
anim_save(&quot;decision_tree_depths.gif&quot;, p)</code></pre>
<p><img src="/regression_trees_pt2/decision_tree_depths.gif" /> <br /> <br /></p>
<p>And what of computational and memory complexity for training and predicting? Let’s disregard the number of columns in <code>df_x</code>, treating it as a constant multiplier, as well as the chosen depth <code>d</code>, supposing it is arbitrarily large, and instead focus on the number of training examples <span class="math inline">\(n\)</span>. While the memory use of <code>train_base</code> may be considered constant (since it is only run as a base case), <code>train_tree</code> makes a full copy of <code>df_x</code> (split across the left and right subsets). If there are <code>k</code> entries less than the chosen threshold, the recurrence relation for memory is thus <span class="math inline">\(S(n) = S(n - k) + S(k) + O(n)\)</span>, which is <span class="math inline">\(O(n^2)\)</span> (if I remember all this theory correctly<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>). But, this is only if <code>k</code> is consistently small or large; if the data are “balanced” such that splits tend to cut the training data in half, the recurrence would be <span class="math inline">\(S(n) = 2S(n/2) + O(n)\)</span>, which is a much better <span class="math inline">\(O(n\log(n))\)</span>.</p>
<p>In terms of computational complexity, <code>split_df</code> is a real hog, because it considers every possible splitting point (<span class="math inline">\(O(n)\)</span>), and computes sum-squared-error for each (also <span class="math inline">\(O(n)\)</span>), resulting in <span class="math inline">\(O(n^2)\)</span>. The recurrence is thus a horrendous <span class="math inline">\(T(n) = T(n-k) + T(k) + O(n^2)\)</span>, or <span class="math inline">\(O(n^3)\)</span>! Well, no one said training machine-learning models was fast. On the other hand, if we could gaurantee that the splits are balanced (by always splitting on the median, say), <code>split_df</code> would be <span class="math inline">\(O(n)\)</span> and the time would be <span class="math inline">\(T(n) = 2T(n/2) + O(n)\)</span> or again <span class="math inline">\(O(n\log(n))\)</span>. One could thus push the splits toward balance, though this would significantly reduce accuracy for unbalanced datasets. An interesting option may be to choose splits <em>randomly</em> (ala quicksort), and use bagging to aggregate these weaker models. (QuickTree?)</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>For classification, the bootstrap aggregation may consider the majority vote of the models’ predictions.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>The Random Forest algorithm is a specific implementation of what is known as the “random subspace method.” The term Random Forest is actually a trademark of Leo Breiman and Adele Cutler, and describes the specific use of both random-column selection and random bootstrap generation.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>The (recommended)[<a href="https://en.wikipedia.org/wiki/Random_forest" class="uri">https://en.wikipedia.org/wiki/Random_forest</a>] size is <span class="math inline">\(\lfloor n/3\rfloor\)</span>, but since we only have two columns in our dataset this isn’t an option.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Note that this is different from simply building a model that makes a crude prediction and adds the actual residuals from the training process, because these residuals also contain all the noise present in the training data.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>We start with the mean of the <code>y</code> values, produced by <code>train_df</code> with <code>depth = 0</code>, rather than a more complex model as above.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>The vignettes for <code>data.tree</code> are excellent and illustrate a number of interesting algorithmic uses for the package. See <code>vignette(&quot;data.tree&quot;)</code> and <code>vignette(&quot;applications&quot;, package = &quot;data.tree&quot;)</code>.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>There’s also the danger of attempting to pin down complexity in a language like R, where computational complexity often hides in simple operations.<a href="#fnref7">↩</a></p></li>
</ol>
</div>
