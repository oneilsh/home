---
banner: "/post/2018-07-25-autodifferntation-and-functional-operators-in-r_files/figure-html/unnamed-chunk-24-1.png"
title: Automatic Differentiation & Functional Operators in R
author: Shawn T. O'Neil
date: '2018-08-08'
slug: autodifferentation-and-functional-operators-in-r
categories: []
tags:
  - programming
  - r
  - theory
  - deep learning
---



<p>I‚Äôve been studying up on deep learning recently (I know, trendy), and I learned something along the way that I think is just incredible.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>First, a little background: deep learning models are artificial neural networks, represented as potentially thousands of nodes with millions of weighted connections between them. Input numbers are fed in to some nodes on one side, and out pops output numbers from some nodes on the other side, after winding through the nodes and weighted connections. The goal is to adjust the connection weights such that the outputs are what we want for any given input.</p>
<center>
<img width="80%" src="https://cdn.edureka.co/blog/wp-content/uploads/2017/05/Deep-Neural-Network-What-is-Deep-Learning-Edureka.png" />
</center>
<center>
<img width="40%" src="https://imgs.xkcd.com/comics/machine_learning.png"/>
</center>
<p>More formally, we define a cost function that compares example model outputs to known good outputs (with the cost being higher for incorrect outputs), and minimize the cost by adjusting the weights. Gradient descent is a very popular method employed for this process.</p>
<p>To make matters easy, let‚Äôs suppose there are just 2 weights, such that the cost function for a given input and output looks like so:</p>
<center>
<img width="50%" src="https://i.imgur.com/B6WiJaI.png"/>
</center>
<p>A given set of weights exists on this surface as a point with a corresponding cost‚Äìmaybe just this side of that big peak. The gradient is the direction that slopes most upward (opposite the direction a ball placed on that point would start to roll). If we know the gradient, we know how we should tweak the weights to lower our cost efficiently. Gradient descent is to iterate this process a number of times until the gradient is zero (which may not be the lowest overall cost, if we fall into a local minimum like the left pit above).</p>
<p>Anyway, gradient descent is all about derivatives: the gradient in the direction of the first weight <span class="math inline">\(w_1\)</span> is the partial derivative of the cost function <span class="math inline">\(f\)</span> with respect to that weight, <span class="math inline">\(\frac{d}{dw_1}f\)</span>. In real deep learning models, this means differentiating a function of thousands or millions of variables, where the function itself is incredibly complex due to all that neural-networky stuff.</p>
<div id="computational-differentation" class="section level3">
<h3>Computational differentation</h3>
<p>For what follows, We‚Äôre going to consider only functions of a single variable, <span class="math inline">\(\mathbb{R} \rightarrow \mathbb{R}\)</span>.</p>
<center>
<img width="30%" src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Quintic_polynomial.svg/1000px-Quintic_polynomial.svg.png"/>
</center>
<p>So, how do we get a computer to differentiate a function? Sure, we can query for the cost value at any particular point. How can we tell what the <em>slope</em> is near there?</p>
<p>Until recently, I only knew of two ways: symbolic differentiation and numerical derivation.</p>
<div id="symbolic-differentation" class="section level4">
<h4>Symbolic differentation</h4>
<p>Symbolic differentiation is basic calculus‚Äìmanipulating function formulas to come up with other function formulas. Some software can do this as well, for example <a href="http://www.wolframalpha.com/input/?i=derivative">Wolfram Alpha</a> can tell me that the <span class="math inline">\(\frac{d}{d x}(2 x + 3 x ^ 3)\)</span> is <span class="math inline">\(2 + 9 x ^2\)</span>. This is pretty cool in itself, because the computer isn‚Äôt executing this code (it‚Äôs not multiplying any actual number by 2), it‚Äôs manipulating the formula as data.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<pre><code>input: D[2 x + 3 x^3, x]
output: 2 + 9 x^2</code></pre>
<p>However, because the functions used in deep learning are so huge and complex, this method isn‚Äôt used, as the formulas can grow exponentially in the size of the network.</p>
</div>
<div id="numerical-differentation" class="section level4">
<h4>Numerical differentation</h4>
<p>This is the straightforward programming way to compute the derivative of a function at a point <span class="math inline">\(x\)</span>, or at least an approximation of it. Supposing <code>f()</code> is the function, and <code>x = 2</code>, we can evaluate the function at <code>x</code> and nearby:</p>
<pre class="r"><code>x &lt;- 2
xnear &lt;- 2.0001
y &lt;- f(x)
ynear &lt;- f(xnear)
slope &lt;- (ynear - y)/(xnear - x)</code></pre>
<p>This doesn‚Äôt work well in practice: if <code>xnear</code> is very close to <code>x</code> then roundoff errors come into play, but if <code>xnear</code> is far from <code>x</code> then the approximation isn‚Äôt very good.</p>
</div>
</div>
<div id="automatic-differentation" class="section level3">
<h3>Automatic differentation</h3>
<p>Automatic differentiation exploits the fact that there are only so many ‚Äúbase‚Äù functions that computers support (modern CPUs support things like <code>+</code> and <code>*</code>, and now even <code>sqrt()</code> and <code>sin()</code>), and generally these have a well-defined derivative function. Further, rules of calculus specify how to compute derivatives of a complicated function based on derivatives of component functions. For example, if we want to compute <span class="math inline">\(\frac{d}{dx}(\sin(x) + \cos(x))\)</span>, this is <span class="math inline">\(\frac{d}{dx}\sin(x) + \frac{d}{dx}\cos(x) = \cos(x) - \sin(x)\)</span>. (The derivative of <span class="math inline">\(\sin(x)\)</span> is <span class="math inline">\(\cos(x)\)</span>, and the derivative of <span class="math inline">\(\cos(x)\)</span> is <span class="math inline">\(-\sin(x)\)</span>.) Thus, by applying derivative rules we eventually get back to known ‚Äúbase‚Äù functions with already-defined derivatives.</p>
<p>The <em>compute</em> above is important: if we want to evaluate the derivative at a given <code>x</code>, and we can work with functions in code this way (add them, subtract them, divide them, etc.), then we can actually <em>call</em> those functions. This is the specialty of functional languages, where functions are themselves types of data that can be operated on (as in symbolic differentiation) and called (as in numerical differentiation).</p>
<p>This is a pretty cool idea, so I fired up my favorite functional language, R, and gave it a shot.</p>
<div id="f-and-f-pairs" class="section level4">
<h4>f() and f‚Äô() pairs</h4>
<p>First, we need easy access to a function and its derivative function: they‚Äôre a matched set. I first thought to hold these in a small R list, starting with <code>\sin()</code> and <code>\cos()</code>, calling them <code>mysin</code> and <code>mycos</code>.</p>
<pre class="r"><code>mycos &lt;- list(f = cos, deriv = sin)
mysin &lt;- list(f = sin, deriv = cos)</code></pre>
<p>This worked to some degree, but to call the function or derivative I needed the awkward syntax <code>mysin$f(x)</code> or <code>mysin$deriv(x)</code>. Worse, there‚Äôs no obvious way to take a second derivative: <code>mysin$deriv(x)</code> is a function in <code>x</code>, but I can‚Äôt get its derivative in an automated way.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>To work around the syntax issue, I made <code>mycos()</code> and <code>mysin()</code> callable functions:</p>
<pre class="r"><code>mycos &lt;- function(x) {
  return(cos(x))
}

mysin &lt;- function(x) {
  return(sin(x))
}</code></pre>
<p>Now, how can we attach the derivative of <code>mysin</code> to <code>mysin</code> itself, a function? Fortunately R allows us to assign ‚Äúattributes‚Äù to any type of data, including functions.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> Ideally the derivative of <code>mysin</code> is <code>mycos</code> rather than just <code>cos</code>; this would allow for second derivatives and higher. My first attempt tried to assign the attributes directly.</p>
<pre class="r"><code>attr(mycos, &quot;getderiv&quot;) &lt;- mysin
attr(mysin, &quot;getderiv&quot;) &lt;- mycos</code></pre>
<p>However, order affects became apparent, because <code>&lt;-</code> is assignment-by-copy. When we run <code>attr(mycos, &quot;getderiv&quot;) &lt;- mysin</code> it‚Äôs attaching a copy of <code>mysin</code> which as of yet doesn‚Äôt have a derivative attached, effectively breaking the ability to compute the second derivative of <code>mycos</code>. Then on <code>attr(mysin, &quot;getderiv&quot;) &lt;- mycos</code>, the <code>mysin</code> function gets its derivative as a copy of <code>mycos</code> that also has a workable first-level derivative. So for <code>mysin</code> we can get first and second derivatives, but not third. (Tl;dr: it didn‚Äôt work.)</p>
<p>Here‚Äôs the solution I landed on. In the <code>&quot;getderiv&quot;</code> attribute, we store an anonymous function whose only job is to return the derivative function. After setting the attribute, we can even call it with <code>attr(mycos, &quot;getderiv&quot;)()</code> syntax.</p>
<pre class="r"><code>attr(mycos, &quot;getderiv&quot;) &lt;- function() {return(mysin)}
attr(mysin, &quot;getderiv&quot;) &lt;- function() {return(mycos)}
deriv &lt;- attr(mysin, &quot;getderiv&quot;)()
print(deriv(c(-1, 0, 1)))</code></pre>
<pre><code>## [1] 0.5403023 1.0000000 0.5403023</code></pre>
<pre class="r"><code>print(cos(c(-1, 0, 1)))</code></pre>
<pre><code>## [1] 0.5403023 1.0000000 0.5403023</code></pre>
<pre class="r"><code>print(deriv)</code></pre>
<pre><code>## function(x) {
##   return(cos(x))
## }
## attr(,&quot;getderiv&quot;)
## function () 
## {
##     return(mysin)
## }</code></pre>
<p>Notice that <code>deriv</code> is a function that does indeed return <code>cos(x)</code>, and furthermore, it itself has a <code>&quot;getderiv&quot;</code> function! This means we can get its derivative:</p>
<pre class="r"><code>deriv_deriv &lt;- attr(deriv, &quot;getderiv&quot;)()
print(deriv_deriv(c(-1, 0, 1)))</code></pre>
<pre><code>## [1] -0.841471  0.000000  0.841471</code></pre>
<pre class="r"><code>print(deriv_deriv)</code></pre>
<pre><code>## function(x) {
##   return(sin(x))
## }
## attr(,&quot;getderiv&quot;)
## function () 
## {
##     return(mycos)
## }</code></pre>
<p>And we can continue to do so, because each function comes packaged with an anonymous function that returns the derivative. These anonymous functions are also closures: <code>function() {return(mysin)}</code> contains <code>mysin</code> bound to (referencing) the data in the calling scope, not a copy (see Appendix below for details). Note that <code>mysin</code> isn‚Äôt evaluated here until the anonymous function is run; otherwise we‚Äôd have an infinite-recursion issue.</p>
<p>Calling <code>attr(mycos, &quot;getderiv&quot;)()</code> is really clunky, so let‚Äôs create a derivative function.</p>
<pre class="r"><code>d &lt;- function(f) {
  deriv &lt;- attr(f, &quot;getderiv&quot;)()
  return(deriv)
}</code></pre>
<p>This is a higher-order function: it takes a function <code>f</code> (which is a function in <code>x</code>) and returns a function (the derivative). This makes good sense, as that‚Äôs what the derivative operator is: a function that maps functions to other functions. To call the derivative, we can now use <code>d(mycos)(x)</code>, which is starting to look as much like mathematical notation as code.</p>
<pre class="r"><code>x &lt;- c(-1, 0, 1)
print(d(mysin)(x))    # calculate and compute derivative of cosin</code></pre>
<pre><code>## [1] 0.5403023 1.0000000 0.5403023</code></pre>
<pre class="r"><code>print(cos(x))       # should be the same</code></pre>
<pre><code>## [1] 0.5403023 1.0000000 0.5403023</code></pre>
<p>We can even do multiple derivatives:</p>
<pre class="r"><code>x &lt;- c(-1, 0, 1)
print(d(d(mycos))(x))    # calculate and compute 2nd derivative of cosin</code></pre>
<pre><code>## [1] 0.5403023 1.0000000 0.5403023</code></pre>
<pre class="r"><code>print(mycos(x))          # should be the same</code></pre>
<pre><code>## [1] 0.5403023 1.0000000 0.5403023</code></pre>
<p>Let‚Äôs see if we can plot <code>mysin(x)</code> and <code>d(mysin)(x)</code> for a sanity check.</p>
<pre class="r"><code>library(ggplot2)
library(tidyr)

# create columns based on function and derivative values
xs &lt;- seq(0, 3*pi, 0.1)
df &lt;- data.frame(x = xs, 
                 fx = mysin(xs), 
                 dfx = d(mysin)(xs))

# reshape for plotting
df_toplot &lt;- gather(df, ytype, value, -x)

p &lt;- ggplot() +
  geom_line(data = df_toplot, aes(x = x, y = value, color = ytype)) 
plot(p)</code></pre>
<p><img src="/post/2018-07-25-autodifferntation-and-functional-operators-in-r_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>So cool.</p>
</div>
<div id="addition-and-subtraction" class="section level4">
<h4>Addition and Subtraction</h4>
<p>Now that we‚Äôve got the representation worked out, let‚Äôs try implementing addition of functions, and derivatives of additions. Mathematically, we want to represent <span class="math inline">\(\frac{d}{dx}(f(x) + g(x)) = \frac{d}{dx}f(x) + \frac{d}{dx}g(x)\)</span>. Here <span class="math inline">\(+\)</span> is also acting as a higher-order function, taking two input functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> and returning the addition. It‚Äôs an ‚Äúinfix‚Äù function, with the function name in the middle of the two parameters.</p>
<p>R lets us create infix functions easily, by using <code>%</code> in the function name. If <code>`%add%` &lt;- function(a, b) { return(a + b)}</code>, then we can call either <code>`%add%`(2, 5)</code> or <code>2 %add% 5</code> to get back <code>7</code>. We just have to use the backticks when defining or using prefix notation.</p>
<p>Let‚Äôs define a <code>%+%</code> function for our functions. As above the result of %+% should also come paired with its derivative, the sum of the functions run through <code>d()</code>.</p>
<pre class="r"><code>`%+%` &lt;- function(f, g) {
  func &lt;- function(x) {
    return(f(x) + g(x))
  }
  
  attr(func, &quot;getderiv&quot;) &lt;- function() {return(d(f) %+% d(g))}
  return(func)
}</code></pre>
<p>Some interesting stuff is happening here: true to form, <code>%+%</code> is a function mapping two input functions in <code>x</code> to a single output function in <code>x</code>. Defined inside, <code>func</code> <em>calls</em> the input functions on the given <code>x</code> (and these are bound via closure). Like <code>mycos</code> and <code>mysin</code> above, the <code>&quot;getderiv&quot;</code> attribute is a function that returns the derivative, and since we create that derivative via <code>d()</code> and the <code>%+%</code> operator, it will also have its own derivative attached. (We‚Äôre working now on the assumption that all higher-order functions we create will take and return such matched-pair functions.) Finally, just as above, <code>d(f) %+% d(g)</code> is not <em>evaluated</em>, just defined with variables bound, so there‚Äôs no infinite recursion. Basically a <a href="https://github.com/tarakc02/lazylist">lazy list</a> of functions.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<p>Let‚Äôs try it.</p>
<pre class="r"><code>print(sin(0.5))</code></pre>
<pre><code>## [1] 0.4794255</code></pre>
<pre class="r"><code>print(cos(0.5))</code></pre>
<pre><code>## [1] 0.8775826</code></pre>
<pre class="r"><code>print(sin(0.5) + cos(0.5))</code></pre>
<pre><code>## [1] 1.357008</code></pre>
<pre class="r"><code>sumfunc &lt;- mysin %+% mycos
print(sumfunc(0.5))</code></pre>
<pre><code>## [1] 1.357008</code></pre>
<pre class="r"><code>print(cos(0.5) + cos(0.5))</code></pre>
<pre><code>## [1] 1.755165</code></pre>
<pre class="r"><code>sumfunc &lt;- mycos %+% d(mysin)    # cos + cos
print(sumfunc(0.5))</code></pre>
<pre><code>## [1] 1.755165</code></pre>
<p>A plot:</p>
<pre class="r"><code>xs &lt;- seq(0, 3*pi, 0.1)
df &lt;- data.frame(x = xs, 
                 fx = mycos(xs), 
                 gx = mysin(xs),
                 sumfxgx = (mycos %+% mysin)(xs))

df_toplot &lt;- gather(df, ytype, value, -x)

p &lt;- ggplot() +
  geom_line(data = df_toplot, aes(x = x, y = value, color = ytype)) 
plot(p)</code></pre>
<p><img src="/post/2018-07-25-autodifferntation-and-functional-operators-in-r_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Let‚Äôs get <code>%-%</code> out of the way while we‚Äôre at it.</p>
<pre class="r"><code>`%-%` &lt;- function(f, g) {
  func &lt;- function(x) {
    return(f(x) - g(x))
  }
  
  attr(func, &quot;getderiv&quot;) &lt;- function() {return(d(f) %-% d(g))}
  return(func)
}</code></pre>
</div>
<div id="multiplication-and-division" class="section level4">
<h4>Multiplication and Division</h4>
<p>Now it‚Äôs time to get brave, and implement <code>%*%</code> with the product rule, <span class="math inline">\(\frac{d}{dx}(f(x) g(x)) = g(x)\frac{d}{dx}f(x) + f(x)\frac{d}{dx}g(x)\)</span>, or more succinctly <span class="math inline">\((f \cdot g)&#39; = f&#39;\cdot g + g&#39;\cdot f\)</span>. In this case the derivative uses all four of <code>d(f)</code>, <code>f</code>, <code>d(g)</code> and <code>g</code>.</p>
<pre class="r"><code>`%*%` &lt;- function(f, g) {
  func &lt;- function(x) {
    return(f(x) * g(x))
  }

  attr(func, &quot;getderiv&quot;) &lt;- function() {return((g %*% d(f)) %+% (f %*% d(g)))}
  return(func)
}</code></pre>
<p>Crossing fingers‚Ä¶</p>
<pre class="r"><code>xs &lt;- seq(0, 3*pi, 0.1)
df &lt;- data.frame(x = xs, 
                 fx = mysin(xs), 
                 gx = mycos(xs),
                 fxgx = (mycos %*% mysin)(xs))

df_toplot &lt;- gather(df, ytype, value, -x)

p &lt;- ggplot() +
  geom_line(data = df_toplot, aes(x = x, y = value, color = ytype)) 
plot(p)</code></pre>
<p><img src="/post/2018-07-25-autodifferntation-and-functional-operators-in-r_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>And <code>%/%</code>, based on <span class="math inline">\((f/g)&#39; = (f&#39; g - g&#39;f)/(g^2)\)</span>.</p>
<pre class="r"><code>`%/%` &lt;- function(f, g) {
  func &lt;- function(x) {
    return(f(x)/g(x))
  }
  
  attr(func, &quot;getderiv&quot;) &lt;- function() {return( ((d(f) %*% g) %-% (d(g) %*% f)) %/% (g %*% g) )}
  return(func)
}</code></pre>
</div>
<div id="constants-and-variables" class="section level4">
<h4>Constants and Variables</h4>
<p>Now, given that we can handle multiplication, can we compute <span class="math inline">\(\frac{d}{dx}2x\)</span> and get back just 2, or do we need a special case? It turns out we can use our <code>%*%</code> function, <em>if we can treat <code>2</code> as a function</em>. Specifically, we want it to be a function in <code>x</code> that always returns 2, no matter the input, since this will allow us to properly combine it with the others.</p>
<p>It will also need an attached derivative. The derivative of a constant is <code>0</code>, also a constant, which we‚Äôll represent in the same way. Let‚Äôs create a <code>const()</code> that returns such a function given the constant of interest; thus <code>const(2)</code> would return one of our functions, as would <code>const(0)</code>.</p>
<pre class="r"><code>const &lt;- function(input) {
  func &lt;- function(x) {
    return(input)
  }
  
  attr(func, &quot;getderiv&quot;) &lt;- function() { return(const(0)) }
  return(func)
}</code></pre>
<p>Here again closures playing an integral role.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> I also like that a <code>const()</code> function‚Äôs derivative is just <code>const(0)</code>, and <code>d()</code> maps <code>const(0)</code> to itself.</p>
<pre class="r"><code>xs &lt;- seq(0, 3*pi, 0.1)
df &lt;- data.frame(x = xs, 
                 fx = (mysin %+% const(2))(xs), 
                 dfx = d(mysin %+% const(2))(xs))

df_toplot &lt;- gather(df, ytype, value, -x)

p &lt;- ggplot() +
  geom_line(data = df_toplot, aes(x = x, y = value, color = ytype)) 
plot(p)</code></pre>
<p><img src="/post/2018-07-25-autodifferntation-and-functional-operators-in-r_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>By the way, footnote readers know that we made a mistake in the definition of <code>mycos</code> earlier, in that <code>d(mycos)</code> should be <code>const(-1) %*% mysin</code> rather than just <code>mysin</code>. Now that we can work with constants like <code>-1</code> we can finally fix it:</p>
<pre class="r"><code>attr(mycos, &quot;getderiv&quot;) &lt;- function() {return(const(-1) %*% mysin)}</code></pre>
<p>We still can‚Äôt quite compute <span class="math inline">\(\frac{d}{dx}2x\)</span> because we don‚Äôt yet have a way to represent <span class="math inline">\(x\)</span>; all we have to work with are functions, and now constants (as functions). Much like our <code>const()</code> function though, we can treat <span class="math inline">\(x\)</span> like a function: one that just returns its input, with a derivative of <code>const(1)</code>.</p>
<pre class="r"><code>xvar &lt;- function(x) {
  return(x)
}

attr(xvar, &quot;getderiv&quot;) &lt;- function() {return(const(1))}</code></pre>
<p>Now we can refer to the <span class="math inline">\(x\)</span> variable as a function by <code>xvar</code>:</p>
<pre class="r"><code>xs &lt;- seq(0, 3*pi, 0.1)
df &lt;- data.frame(x = xs, 
                 fx = (const(2) %*% xvar)(xs), 
                 dfx = d(const(2) %*% xvar)(xs))

df_toplot &lt;- gather(df, ytype, value, -x)

p &lt;- ggplot() +
  geom_line(data = df_toplot, aes(x = x, y = value, color = ytype)) 
plot(p)</code></pre>
<p><img src="/post/2018-07-25-autodifferntation-and-functional-operators-in-r_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div id="composition" class="section level4">
<h4>Composition</h4>
<p>What about something like <span class="math inline">\(\frac{d}{dx}\sin(\cos(x))\)</span>? Mathematically this is derived with the chain rule <span class="math inline">\(\frac{d}{dx}f(g(x)) = f&#39;(g(x))g&#39;(x)\)</span>. But <span class="math inline">\(\sin(\cos(x))\)</span> is difficult to represent in our scheme, since <code>mysin(mycos)</code> would nonsensically attempt to call <code>mysin</code> on the <code>mycos</code> function. What we need is the composition operator, <span class="math inline">\(\circ\)</span>, which is the infix version of calling <span class="math inline">\(f\)</span> on the result of <span class="math inline">\(g\)</span>: <span class="math inline">\((f \circ g)(x) = f(g(x))\)</span>. We‚Äôll use <code>%o%</code>:</p>
<pre class="r"><code>`%o%` &lt;- function(f, g) {
  func &lt;- function(x) {
    return(f(g(x)))
  }
  
  attr(func, &quot;getderiv&quot;) &lt;- function() {return(  (d(f) %o% g) %*% d(g) )}
  return(func)
}</code></pre>
<p>Let‚Äôs have fun with <span class="math inline">\(\sin(2\cos(x))\)</span> and its first and second derivatives.</p>
<pre class="r"><code>xs &lt;- seq(0, 3*pi, 0.1)
df &lt;- data.frame(x = xs, 
                 fx = (mysin %o% (const(2) %*% mycos))(xs), 
                 dfx = d(mysin %o% (const(2) %*% mycos))(xs),
                 ddfx = d(d(mysin %o% (const(2) %*% mycos)))(xs))

df_toplot &lt;- gather(df, ytype, value, -x) 

p &lt;- ggplot() +
  geom_line(data = df_toplot, aes(x = x, y = value, color = ytype)) 
plot(p)</code></pre>
<p><img src="/post/2018-07-25-autodifferntation-and-functional-operators-in-r_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
<div id="power-rules" class="section level4">
<h4>Power Rules</h4>
<p>Rather than implement the power rule, <span class="math inline">\(\frac{d}{dx}x^n = nx^{n-1}\)</span>, or even the <em>general</em> power rule, <span class="math inline">\(\frac{d}{dx}f(x)^n = nf(x)^{n-1}\frac{d}{dx}f(x)\)</span>, we‚Äôll go straight for the functional general<em>ized</em> power rule: <span class="math inline">\((f^g)&#39; = -f^g\left( f&#39; \frac{g}{f} + g&#39; \ln f \right)\)</span>. This version derivates a function raised to the power of another, as in <span class="math inline">\(\frac{d}{dx}\cos(x)^{\sin(s)}\)</span>.</p>
<p>First though, we need to implement the natural logarithm, whose derivative is <span class="math inline">\(\frac{d}{dx}\ln(x) = 1/x\)</span>. (R uses <code>log()</code> for natural logarithm.)</p>
<pre class="r"><code>myln &lt;- function(x) {
  return(log(x))
}
attr(myln, &quot;getderiv&quot;) &lt;- function() {return(const(1) %/% xvar)}


`%^%` &lt;- function(g, h) {
  func &lt;- function(x) {
    return(g(x) ^ h(x))
  }

  attr(func, &quot;getderiv&quot;) &lt;- function() {return(  
                                           const(-1) %*% (g %^% h) %*%
                                                   (  
                                                     (d(h) %*% (myln %o% g)) %+%
                                                     (h %*% (d(g) %/% g))
                                                   )
                                              )} 
  return(func)
}</code></pre>
<p>This example only plots <span class="math inline">\(\cos^3(x)\)</span> and <span class="math inline">\(\frac{d}{dx}\cos^3(x)\)</span>, but since <code>const(3)</code> is a function in <code>x</code> it fits in this scheme too.</p>
<pre class="r"><code>xs &lt;- seq(0, 3*pi, 0.1)
df &lt;- data.frame(x = xs, 
                 fx = (mycos %^% const(3))(xs), 
                 dfx = d(mycos %^% const(3))(xs))</code></pre>
<pre><code>## Warning in log(x): NaNs produced</code></pre>
<pre class="r"><code>df_toplot &lt;- gather(df, ytype, value, -x) 

p &lt;- ggplot() +
  geom_line(data = df_toplot, aes(x = x, y = value, color = ytype)) 
plot(p)</code></pre>
<pre><code>## Warning: Removed 16 rows containing missing values (geom_path).</code></pre>
<p><img src="/post/2018-07-25-autodifferntation-and-functional-operators-in-r_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>When <span class="math inline">\(f(x)\)</span> is negative the derivative isn‚Äôt defined according to the generalized power rule, because the logarithm isn‚Äôt defined for negative numbers. The general power rule would work, but can only accept a <code>const()</code> as the power.</p>
<p>Speaking of logarithms and exponents, here‚Äôs <span class="math inline">\(e^x\)</span>, which is its own derivative:</p>
<pre class="r"><code># this one is too dang easy ;)
myexp &lt;- function(x) {
  return(exp(x))
}
attr(myexp, &quot;getderiv&quot;) &lt;- function() {return(myexp)}</code></pre>
</div>
</div>
<div id="efficiency" class="section level3">
<h3>Efficiency</h3>
<p>By my understanding, there are two types of automatic differentiation. The first, forward accumulation, is what we‚Äôve implemented: derivatives are computed for functions in terms of their component functions, recursively. In the second method, reverse accumulation, derivatives of basic elements are computed, and used &amp; re-used for computing derivatives of their more complex parents.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
<p>Reverse mode has two primary advantages: first, that it is inherently vectorized in cases where the function is of multiple variables such as <span class="math inline">\(f(x,y) = x + y\)</span>, computing the gradient vector <span class="math inline">\(\left&lt;\frac{d}{dx}f, \frac{d}{dy}f\right&gt;\)</span> in one ‚Äúpass‚Äù. Forward mode can only compute one partial derivative <span class="math inline">\(\frac{d}{dx}f\)</span> or <span class="math inline">\(\frac{d}{dy}f\)</span> in a single pass. (We haven‚Äôt needed to do this, working solely with functions of one variable.) The second advantage of reverse mode is that derivatives of functions are re-used if they are useful for multiple functions needing them.</p>
<p>I‚Äôm not sure what could be done about the first disadvantage of forward mode, but perhaps we can do something about the second. Consider computing a complex expression for a particular value:</p>
<pre class="r"><code>f &lt;- ((mycos %*% mysin) %+% (mycos %*% mysin)) %/% 
     ((mycos %*% mysin) %+% (mycos %*% mysin))
print(f(1.5))</code></pre>
<pre><code>## [1] 1</code></pre>
<p>When this function is called, it recursively computes the component pieces. We can see this more directly if we add lines like <code>print(&quot;called %+%&quot;)</code> inside of <code>%+%</code> et al, and then run <code>print(f(1.5))</code>. I‚Äôve indented the lines to show the nesting:</p>
<pre><code>## [1] &quot;called %/%&quot;
    ## [1] &quot;called %+%&quot;
        ## [1] &quot;called %*%&quot;
            ## [1] &quot;called mycos&quot;
            ## [1] &quot;called mysin&quot;
        ## [1] &quot;called %*%&quot;
            ## [1] &quot;called mycos&quot;
            ## [1] &quot;called mysin&quot;
    ## [1] &quot;called %+%&quot;
        ## [1] &quot;called %*%&quot;
            ## [1] &quot;called mycos&quot;
            ## [1] &quot;called mysin&quot;
        ## [1] &quot;called %*%&quot;
            ## [1] &quot;called mycos&quot;
            ## [1] &quot;called mysin&quot;
## [1] 1</code></pre>
<p>This example is setup for repeated computation: <code>mycos %*% mysin</code> is computed four times total, twice within the first <code>%+%</code> and twice within the second <code>%+%</code>. Computing <code>d(f)(1.5)</code> required 72 calls! A technique called <a href="https://en.wikipedia.org/wiki/Memoization">memoization</a> caches the results of function calls; before a function is called the cache is checked to see if the result has already been computed. The <code>memoise</code> package allows us to easily cache results for our functions by wrapping them in <code>memoise(  )</code>.</p>
<pre class="r"><code>library(memoise)

`%+%` &lt;- memoise(
  function(f, g) {
    print(&quot;called %+%&quot;)
    func &lt;- function(x) {
      return(f(x) + g(x))
    }
    
    attr(func, &quot;getderiv&quot;) &lt;- function() {return(d(f) %+% d(g))}
    return(func)
  }
)</code></pre>
<p>The result is that identical sub-functions are reused. Once the need for something like <code>mycos %*% mysin</code> arises, the cache is consulted before the function is called. If the answer is present in the cache, it is used and the function is never called. Here‚Äôs the result of <code>print(f(1.5))</code>:</p>
<pre><code>## [1] &quot;called %*%&quot;
## [1] &quot;called %+%&quot;
## [1] &quot;called %/%&quot;
## [1] &quot;called mycos&quot;
## [1] &quot;called mysin&quot;
## [1] 1</code></pre>
<p>The derivative <code>d(f)(1.5)</code> is down to 23 calls from 72.</p>
</div>
<div id="summary" class="section level3">
<h3>Summary</h3>
<p>Automatic differentiation is a really interesting technique, and it highlights some of the unique features of functional programming. So many operations in mathematics are higher-order functions‚ÄìI suppose it‚Äôs no surprise that lisp (the basis for <a href="https://en.wikipedia.org/wiki/Scheme_(programming_language)">scheme</a>, which <a href="https://cran.r-project.org/doc/html/interface98-paper/paper_1.html">inspired R</a>) was based on the <a href="https://en.wikipedia.org/wiki/Lambda_calculus">lambda calculus</a>, developed by mathematician Alonzo Church in the 1930‚Äôs.</p>
<p>There‚Äôs still a lot left to explore, such as functions of multiple variables, gradients (series of partial derivatives), and gradient-descent. Perhaps I‚Äôll be able to cover those in another post.</p>
<p>Some interesting links for those wishing to learn more:</p>
<p>Forward and reverse differentiation, with some example code in Rust and Python: <a href="http://www.columbia.edu/~ahd2125/post/2015/12/5/" class="uri">http://www.columbia.edu/~ahd2125/post/2015/12/5/</a></p>
<p>Another article with code examples, also Rust and Python: <a href="https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation" class="uri">https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation</a> The tape-based implementation near the end is interesting.</p>
<p>A recent article on a novel, simpler method for automatic differentiation with an implementation library in Haskell: <a href="https://arxiv.org/pdf/1804.00746.pdf" class="uri">https://arxiv.org/pdf/1804.00746.pdf</a>. This project has been pretty difficult to debug, with all the recursive anonymous functions, lazy evaluation, and especially the untyped nature of R. I can easily see how a rich type system such Haskell‚Äôs would be helpful.</p>
<div id="acknowledgements" class="section level4">
<h4>Acknowledgements</h4>
<p>I‚Äôd like to thank Tarak Shah (<a href="https://twitter.com/tarakc02">@tarakc02</a>) for helpful comments. Thanks Tarak!</p>
<div style="width: 100%; height: 250px;">

</div>
</div>
</div>
<div id="appendix" class="section level3">
<h3>Appendix</h3>
<p>A quick note on closures: closures are functions that make use of non-local data. Often these are found as ‚Äúglobal variables‚Äù that should be passed as parameters to the function to make it more portable.</p>
<pre class="r"><code>c &lt;- 36

add &lt;- function(x, y) {
  sum &lt;- x + y + c       # x and y are local variables, but c is global
  return(sum)            # ...gross
}

print(add(100, 200))     # prints 336 </code></pre>
<p>Interestingly, variables scopes can be nested:</p>
<pre class="r"><code>a &lt;- 100            # a is available globally, and to get_inner_func and inner_func

get_inner_func &lt;- function() {
  b &lt;- 200 + a      # b is available to get_inner_func and inner_func
  
  inner_func &lt;- function() {
    c &lt;- 300 + b    # c is available to inner_func
    return(c)
  }
  
  return(inner_func)
}

inner_func &lt;- get_inner_func()
inner_func()</code></pre>
<p>What‚Äôs more, closures ‚Äúcarry with them‚Äù the scope (or environment) they were defined in. So in the above, the function returned by <code>get_inner_func()</code> has it‚Äôs own local <code>c</code>, as well as still access to <code>b</code> and <code>a</code>. While accessing variables outside a function‚Äôs scope can be risky (they get very hard to debug), it is occasionally very useful as we see here.</p>
<p>There are a couples ways to do closures though: is the environment carried with the function a static snapshot, or subject to change as other functions modify the underlying data? The former is sometimes called ‚Äòclose-by-value‚Äô or ‚Äò<a href="https://en.wikipedia.org/wiki/Closure_(computer_programming)#Lexical_environment">capture-by-value</a>,‚Äô and the latter sometimes called ‚Äòclose-by-reference‚Äô or ‚Äòcapture-by-reference.‚Äô R is capture-by-reference. (As some may know, R environments provide one of the few options for reference mechanics in the language.)</p>
<p>Here‚Äôs a quick verification of R‚Äôs close-by-reference nature, based on <a href="https://www.quora.com/Do-closures-in-JavaScript-make-a-copy-of-the-parent-scope-variables-or-just-refer-to-them">this Quora answer</a>. Here <code>a</code> is in the scope of <code>make_inc_get()</code>, while <code>inc_a()</code> and <code>get_a()</code> both use it. In <code>inc_a</code> we use the <code>&lt;&lt;-</code> assignment operator, which assigns ‚Äúup‚Äù the scope hierarchy to the first match, whereas <code>&lt;-</code> creates a local variable.</p>
<pre class="r"><code>make_inc_get &lt;- function() {
  a &lt;- 0; 
  
  inc_a &lt;- function() {
    a &lt;&lt;- a + 1
  }
  
  get_a &lt;- function() {
    return(a)
  }
  
  return(list(inc_a = inc_a, get_a = get_a))
}

inc_get &lt;- make_inc_get()
inc_get$inc_a()
print(inc_get$get_a())</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>inc_get$inc_a()
print(inc_get$get_a())</code></pre>
<pre><code>## [1] 2</code></pre>
<pre class="r"><code>inc_get$inc_a()
print(inc_get$get_a())</code></pre>
<pre><code>## [1] 3</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I should note that I‚Äôm not an expert in any of these topics‚Äìjust an enthusiast üòÅ The automatic differention here I‚Äôm sure bears little resemblance to real engines such as TensorFlow.<a href="#fnref1">‚Ü©</a></p></li>
<li id="fn2"><p>‚Ä¶ as far as I know. The Structure and Interpretation of Computer Languages (<a href="http://web.mit.edu/alexmv/6.037/sicp.pdf">SICP</a>) explores this.<a href="#fnref2">‚Ü©</a></p></li>
<li id="fn3"><p>You may also notice I‚Äôve made a mistake, because the derivative of <span class="math inline">\(\cos(x)\)</span> should be <span class="math inline">\(-\sin(x)\)</span>, not <span class="math inline">\(\sin(x)\)</span>. However, for the sake of exposition, I‚Äôm going to leave it for now: we don‚Äôt yet have a clear way to define the negation of a function. <code>-1 * sin</code> won‚Äôt work, as <code>*</code> isn‚Äôt defined for a number and a function.<a href="#fnref3">‚Ü©</a></p></li>
<li id="fn4"><p>JavaScript also allows properties on functions since functions are objects. I think most of this would work very similarly in JavaScript.<a href="#fnref4">‚Ü©</a></p></li>
<li id="fn5"><p>This took me a while to figure out. Initially I was defining <code>deriv &lt;- function(x) {return(d(f)(x) + d(g)(x))}</code>, and then <code>attr(func, &quot;getderiv&quot;) &lt;- function() {return(deriv)}</code>. This only allows the first derivative however, because <code>deriv</code> isn‚Äôt itself a matched-pair function.<a href="#fnref5">‚Ü©</a></p></li>
<li id="fn6"><p>Sorry, that joke is a little derivative.<a href="#fnref6">‚Ü©</a></p></li>
<li id="fn7"><p>A more useful overview with code examples can be found <a href="https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation">here</a>.<a href="#fnref7">‚Ü©</a></p></li>
</ol>
</div>
